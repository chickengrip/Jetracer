{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983b24af",
   "metadata": {},
   "source": [
    "# Interactive Image Classification: Drive vs Stop\n",
    "\n",
    "This notebook provides an interactive workflow for collecting, training, and deploying a binary image classifier that distinguishes between \"drive\" and \"stop\" using live camera input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be86c4a",
   "metadata": {},
   "source": [
    "## 1. Camera Initialization\n",
    "\n",
    "Import and initialize the camera for capturing live images. You can use either the CSI or USB camera depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca894ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetcam.csi_camera import CSICamera\n",
    "# from jetcam.usb_camera import USBCamera\n",
    "\n",
    "camera = CSICamera(width=224, height=224)\n",
    "# camera = USBCamera(width=224, height=224)\n",
    "\n",
    "camera.running = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f688f2",
   "metadata": {},
   "source": [
    "## 2. Task and Dataset Preparation\n",
    "\n",
    "Define the classification task, set up the two classes ('drive', 'stop'), and prepare the dataset and transforms for image preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e4d0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from xy_dataset import XYDataset  # You may need to adapt this or use a custom dataset for classification\n",
    "\n",
    "TASK = 'drive_stop_classification'\n",
    "CLASSES = ['drive', 'stop']\n",
    "DATASETS = ['A', 'B']\n",
    "\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# You may need to implement a simple classification dataset if XYDataset is for regression\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, root, classes, transform=None):\n",
    "        self.root = root\n",
    "        self.classes = classes\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        for idx, cls in enumerate(classes):\n",
    "            cls_dir = os.path.join(root, cls)\n",
    "            if os.path.exists(cls_dir):\n",
    "                for fname in os.listdir(cls_dir):\n",
    "                    if fname.endswith('.jpg') or fname.endswith('.png'):\n",
    "                        self.samples.append((os.path.join(cls_dir, fname), idx))\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    def add_sample(self, class_name, image_array):\n",
    "        cls_dir = os.path.join(self.root, class_name)\n",
    "        os.makedirs(cls_dir, exist_ok=True)\n",
    "        idx = len(os.listdir(cls_dir))\n",
    "        fname = os.path.join(cls_dir, f\"{idx:04d}.jpg\")\n",
    "        Image.fromarray(image_array).save(fname)\n",
    "        self.samples.append((fname, self.classes.index(class_name)))\n",
    "    def get_count(self, class_name):\n",
    "        cls_dir = os.path.join(self.root, class_name)\n",
    "        if os.path.exists(cls_dir):\n",
    "            return len(os.listdir(cls_dir))\n",
    "        return 0\n",
    "\n",
    "datasets = {}\n",
    "for name in DATASETS:\n",
    "    datasets[name] = ClassificationDataset(TASK + '_' + name, CLASSES, TRANSFORMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16c7f8f",
   "metadata": {},
   "source": [
    "## 3. Data Collection Widget for Classification\n",
    "\n",
    "Create widgets to capture images from the camera and label them as either 'drive' or 'stop'. Save labeled images to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f601a83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff91d610d12047a698e1d1b7778a810c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(ClickableImageWidget(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "from jupyter_clickable_image_widget import ClickableImageWidget\n",
    "\n",
    "# initialize active dataset\n",
    "dataset = datasets[DATASETS[0]]\n",
    "\n",
    "# unobserve all callbacks from camera in case we are running this cell for second time\n",
    "camera.unobserve_all()\n",
    "\n",
    "# create image preview\n",
    "camera_widget = ClickableImageWidget(width=camera.width, height=camera.height)\n",
    "snapshot_widget = ipywidgets.Image(width=camera.width, height=camera.height)\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# create widgets\n",
    "dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='dataset')\n",
    "class_widget = ipywidgets.Dropdown(options=CLASSES, description='class')\n",
    "count_widget = ipywidgets.IntText(description='count')\n",
    "\n",
    "# manually update counts at initialization\n",
    "count_widget.value = dataset.get_count(class_widget.value)\n",
    "\n",
    "def set_dataset(change):\n",
    "    global dataset\n",
    "    dataset = datasets[change['new']]\n",
    "    count_widget.value = dataset.get_count(class_widget.value)\n",
    "dataset_widget.observe(set_dataset, names='value')\n",
    "\n",
    "def update_counts(change):\n",
    "    count_widget.value = dataset.get_count(change['new'])\n",
    "class_widget.observe(update_counts, names='value')\n",
    "\n",
    "def save_snapshot(_=None):\n",
    "    # save to disk\n",
    "    dataset.add_sample(class_widget.value, camera.value)\n",
    "    # display saved snapshot\n",
    "    snapshot = camera.value.copy()\n",
    "    snapshot_widget.value = bgr8_to_jpeg(snapshot)\n",
    "    count_widget.value = dataset.get_count(class_widget.value)\n",
    "\n",
    "save_button = ipywidgets.Button(description='Save Image')\n",
    "save_button.on_click(lambda _: save_snapshot())\n",
    "\n",
    "data_collection_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([camera_widget, snapshot_widget]),\n",
    "    dataset_widget,\n",
    "    class_widget,\n",
    "    count_widget,\n",
    "    save_button\n",
    "])\n",
    "\n",
    "display(data_collection_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98e8920",
   "metadata": {},
   "source": [
    "## 4. Model Definition and Management\n",
    "\n",
    "Define a neural network model (e.g., ResNet18) for binary classification, and provide widgets to save/load model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298f23e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af7691bef594e71a6e8ffdde9753c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='drive_stop_model.pth', description='model path'), HBox(children=(Button(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "output_dim = 2  # drive, stop\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, output_dim)\n",
    "model = model.to(device)\n",
    "\n",
    "model_save_button = ipywidgets.Button(description='save model')\n",
    "model_load_button = ipywidgets.Button(description='load model')\n",
    "model_path_widget = ipywidgets.Text(description='model path', value='drive_stop_model.pth')\n",
    "\n",
    "def load_model(c):\n",
    "    model.load_state_dict(torch.load(model_path_widget.value, map_location=device))\n",
    "model_load_button.on_click(load_model)\n",
    "    \n",
    "def save_model(c):\n",
    "    torch.save(model.state_dict(), model_path_widget.value)\n",
    "model_save_button.on_click(save_model)\n",
    "\n",
    "model_widget = ipywidgets.VBox([\n",
    "    model_path_widget,\n",
    "    ipywidgets.HBox([model_load_button, model_save_button])\n",
    "])\n",
    "\n",
    "display(model_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da33b8",
   "metadata": {},
   "source": [
    "## 5. Live Classification Execution\n",
    "\n",
    "Implement a live preview that runs the model on camera images and displays the predicted class in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb57c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f396391090e4d92989caf08aaf8de70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Image(value=b'', format='jpeg', height='224', width='224'), Label(value='Prediction: -'), Toggl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "from torchvision import transforms as T\n",
    "\n",
    "state_widget = ipywidgets.ToggleButtons(options=['stop', 'live'], description='state', value='stop')\n",
    "prediction_widget = ipywidgets.Image(format='jpeg', width=camera.width, height=camera.height)\n",
    "predicted_class_widget = ipywidgets.Label(value=\"Prediction: -\")\n",
    "\n",
    "preprocess = TRANSFORMS\n",
    "\n",
    "def live_classification(state_widget, model, camera, prediction_widget, predicted_class_widget):\n",
    "    while state_widget.value == 'live':\n",
    "        image = camera.value\n",
    "        input_image = Image.fromarray(image)\n",
    "        input_tensor = preprocess(input_image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            pred_idx = torch.argmax(output, dim=1).item()\n",
    "            pred_class = CLASSES[pred_idx]\n",
    "        # Draw predicted class on image\n",
    "        display_image = image.copy()\n",
    "        cv2.putText(display_image, pred_class, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
    "        prediction_widget.value = bgr8_to_jpeg(display_image)\n",
    "        predicted_class_widget.value = f\"Prediction: {pred_class}\"\n",
    "        time.sleep(0.1)\n",
    "\n",
    "def start_live(change):\n",
    "    if change['new'] == 'live':\n",
    "        execute_thread = threading.Thread(target=live_classification, args=(state_widget, model, camera, prediction_widget, predicted_class_widget))\n",
    "        execute_thread.start()\n",
    "\n",
    "state_widget.observe(start_live, names='value')\n",
    "\n",
    "live_execution_widget = ipywidgets.VBox([\n",
    "    prediction_widget,\n",
    "    predicted_class_widget,\n",
    "    state_widget\n",
    "])\n",
    "\n",
    "display(live_execution_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03f57df",
   "metadata": {},
   "source": [
    "## 6. Training and Evaluation Controls\n",
    "\n",
    "Add widgets to control training (epochs, train/evaluate buttons, progress/loss display) and implement the training/evaluation loop for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c70c4c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7fb8fad187c45e1a39b70edf449ca0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntText(value=1, description='epochs'), FloatProgress(value=0.0, description='progress', max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "epochs_widget = ipywidgets.IntText(description='epochs', value=1)\n",
    "eval_button = ipywidgets.Button(description='evaluate')\n",
    "train_button = ipywidgets.Button(description='train')\n",
    "loss_widget = ipywidgets.FloatText(description='loss')\n",
    "progress_widget = ipywidgets.FloatProgress(min=0.0, max=1.0, description='progress')\n",
    "accuracy_widget = ipywidgets.FloatText(description='accuracy')\n",
    "\n",
    "def train_eval(is_training):\n",
    "    global BATCH_SIZE, model, dataset, optimizer, eval_button, train_button, accuracy_widget, loss_widget, progress_widget, state_widget\n",
    "\n",
    "    try:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        state_widget.value = 'stop'\n",
    "        train_button.disabled = True\n",
    "        eval_button.disabled = True\n",
    "        time.sleep(1)\n",
    "\n",
    "        if is_training:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        for epoch in range(epochs_widget.value if is_training else 1):\n",
    "            i = 0\n",
    "            sum_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in train_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                if is_training:\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if is_training:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "                i += labels.size(0)\n",
    "                sum_loss += float(loss)\n",
    "                progress_widget.value = i / len(dataset)\n",
    "                loss_widget.value = sum_loss / i\n",
    "                accuracy_widget.value = correct / total\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    model.eval()\n",
    "\n",
    "    train_button.disabled = False\n",
    "    eval_button.disabled = False\n",
    "    state_widget.value = 'live'\n",
    "\n",
    "train_button.on_click(lambda c: train_eval(is_training=True))\n",
    "eval_button.on_click(lambda c: train_eval(is_training=False))\n",
    "\n",
    "train_eval_widget = ipywidgets.VBox([\n",
    "    epochs_widget,\n",
    "    progress_widget,\n",
    "    loss_widget,\n",
    "    accuracy_widget,\n",
    "    ipywidgets.HBox([train_button, eval_button])\n",
    "])\n",
    "\n",
    "display(train_eval_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc536634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:no value was provided for `target_layer`, thus set to 'layer4'.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2824a792",
   "metadata": {},
   "source": [
    "## 7. Combined Widget Display\n",
    "\n",
    "Combine all widgets into a single interface for streamlined data collection, training, and live inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef662db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e3949819f7424787d030b5e95efec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(VBox(children=(HBox(children=(ClickableImageWidget(value=b'\\xff\\xd8\\xff\\xe0\\x00\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([data_collection_widget, live_execution_widget]), \n",
    "    train_eval_widget,\n",
    "    model_widget\n",
    "])\n",
    "\n",
    "display(all_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5dea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
